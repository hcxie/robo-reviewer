{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Copyright 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved.<br>\n",
    "This is AWS Content subject to the terms of the Customer Agreement</small>\n",
    "\n",
    "# Module 2.1: Video Frame Processing Walkthrough\n",
    "\n",
    "This notebook demonstrates video frame extraction and processing using different sampling methods. We'll explore both traditional sampling (middle/random) and advanced semantic sampling for intelligent frame selection.\n",
    "\n",
    "## A. Overview\n",
    "\n",
    "The video frame processing pipeline consists of:\n",
    "1. **Traditional Sampling** - Middle and random frame selection\n",
    "2. **Semantic Sampling** - AI-powered scene detection and frame selection\n",
    "3. **Batch Processing** - Process multiple videos from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Install Dependencies\n",
    "\n",
    "First, let's install the required packages for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib opencv-python Pillow tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Setup and Imports\n",
    "\n",
    "First, we'll import all the necessary libraries for video processing, image manipulation, and AWS services. These imports provide the foundation for frame extraction, embedding generation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.video_processing import visualize_frames, process_videos_pipeline\n",
    "from utils.config import get_s3_bucket, discover_video_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Configuration\n",
    "\n",
    "Configure your AWS session and video processing parameters. Make sure your video file exists in the specified path and adjust the number of frames based on your analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "session = boto3.Session()\n",
    "\n",
    "# Video processing parameters\n",
    "VIDEO_PATH1 = \"generated_videos/video_1.mp4\"  # Local video file\n",
    "VIDEO_PATH2 = \"generated_videos/video_2.mp4\"  # Local video file\n",
    "NUM_FRAMES = 16  # Number of frames to extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Extract All Frames from Video\n",
    "\n",
    "This section demonstrates how to load a video file and extract all individual frames using OpenCV. We'll convert the video into a list of frame arrays that can be processed by our sampling algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all frames from a video file\n",
    "def convert_to_frames(video_path):\n",
    "    \"\"\"\n",
    "    Extract all frames from a video file using OpenCV.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of frame arrays (numpy arrays)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_indices_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # Extract all frames from video\n",
    "    frames = []\n",
    "    for i in range(frame_indices_len):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frames.append(frame)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video and extract all frames\n",
    "# This creates a list of numpy arrays, one for each frame\n",
    "frames = convert_to_frames(VIDEO_PATH1)\n",
    "print(f\"ðŸ“¹ Extracted {len(frames)} frames from video1\")\n",
    "\n",
    "frames_long = convert_to_frames(VIDEO_PATH2)\n",
    "print(f\"ðŸ“¹ Extracted {len(frames_long)} frames from video2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Traditional Frame Sampling (Uniform/Random)\n",
    "\n",
    "Traditional sampling methods divide the video into equal intervals and select frames systematically. **Uniform sampling** selects the middle frame from each interval for consistent representation, while **random sampling** picks frames randomly within each interval for variety. These methods are fast and work well for videos with consistent content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for uniform frame sampling - selects middle frame from equal intervals\n",
    "def get_frame_indices_uniform(num_frames, frames):\n",
    "    \"\"\"\n",
    "    Sample frames uniformly by selecting the middle frame from equal intervals.\n",
    "    \n",
    "    Args:\n",
    "        num_frames (int): Number of frames to sample\n",
    "        frames (list): List of all video frames\n",
    "        \n",
    "    Returns:\n",
    "        list: List of sampled frames\n",
    "    \"\"\"\n",
    "    total_frames = len(frames)\n",
    "    acc_samples = min(num_frames, total_frames)\n",
    "    intervals = np.linspace(\n",
    "        start=0, \n",
    "        stop=total_frames, \n",
    "        num=acc_samples + 1\n",
    "    ).astype(int)\n",
    "    # Create frame ranges for each interval\n",
    "    ranges = []\n",
    "    for idx, interv in enumerate(intervals[:-1]):\n",
    "        ranges.append((interv, intervals[idx + 1] - 1))\n",
    "\n",
    "    frame_indices = [(x[0] + x[1]) // 2 for x in ranges]\n",
    "\n",
    "    if len(frame_indices) < num_frames:\n",
    "        padded_frame_indices = [frame_indices[-1]] * num_frames\n",
    "        padded_frame_indices[:len(frame_indices)] = frame_indices\n",
    "        frame_indices = padded_frame_indices\n",
    "\n",
    "    return [frames[idx] for idx in frame_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for random frame sampling - randomly selects frames from equal intervals\n",
    "def get_frame_indices_random(num_frames, frames):\n",
    "    \"\"\"\n",
    "    Sample frames randomly by selecting random frames from equal intervals.\n",
    "    \n",
    "    Args:\n",
    "        num_frames (int): Number of frames to sample\n",
    "        frames (list): List of all video frames\n",
    "        \n",
    "    Returns:\n",
    "        list: List of randomly sampled frames\n",
    "    \"\"\"\n",
    "    total_frames = len(frames)\n",
    "    acc_samples = min(num_frames, total_frames)\n",
    "    intervals = np.linspace(\n",
    "        start=0, \n",
    "        stop=total_frames, \n",
    "        num=acc_samples + 1\n",
    "    ).astype(int)\n",
    "    # Create frame ranges for each interval\n",
    "    ranges = []\n",
    "    for idx, interv in enumerate(intervals[:-1]):\n",
    "        ranges.append((interv, intervals[idx + 1] - 1))\n",
    "\n",
    "    # Randomly select one frame from each interval\n",
    "    try:\n",
    "        frame_indices = [random.choice(range(x[0], x[1])) for x in ranges]\n",
    "    except:\n",
    "        # Fallback: random permutation if ranges are invalid\n",
    "        frame_indices = np.random.permutation(total_frames)[:acc_samples]\n",
    "        frame_indices.sort()\n",
    "        frame_indices = list(frame_indices)\n",
    "\n",
    "    if len(frame_indices) < num_frames:\n",
    "        padded_frame_indices = [frame_indices[-1]] * num_frames\n",
    "        padded_frame_indices[:len(frame_indices)] = frame_indices\n",
    "        frame_indices = padded_frame_indices\n",
    "\n",
    "    return [frames[idx] for idx in frame_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply uniform sampling to select evenly distributed frames\n",
    "# This method ensures consistent temporal coverage across the video\n",
    "uniform_sampled_frames = get_frame_indices_uniform(NUM_FRAMES, frames)\n",
    "print(f\"ðŸŽ¯ Uniform sampling selected {len(uniform_sampled_frames)} frames\")\n",
    "visualize_frames(uniform_sampled_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random sampling for varied frame selection\n",
    "# This method introduces randomness while maintaining temporal distribution\n",
    "rand_sampled_frames = get_frame_indices_random(NUM_FRAMES, frames)\n",
    "print(f\"ðŸŽ² Random sampling selected {len(rand_sampled_frames)} frames\")\n",
    "visualize_frames(rand_sampled_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Semantic Frame Sampling\n",
    "\n",
    "Semantic sampling uses AI to understand video content and intelligently select frames. It generates embeddings for each frame using Amazon Titan, calculates similarities between consecutive frames, and identifies scene boundaries where content changes significantly. This method ensures we capture the most representative frames from each distinct scene or segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embedding for a single image using Amazon Titan\n",
    "def get_titan_embedding_image(\n",
    "        boto3_session, \n",
    "        image_base64\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generate image embedding using Amazon Titan Embed Image model.\n",
    "    \n",
    "    Args:\n",
    "        boto3_session: AWS boto3 session for API calls\n",
    "        image_base64 (str): Base64 encoded image string\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 1024-dimensional embedding vector reshaped to (1, 1024)\n",
    "        None: If all retry attempts fail\n",
    "    \"\"\"\n",
    "    # Initialize Bedrock runtime client\n",
    "    bedrock_client = boto3_session.client(service_name='bedrock-runtime')\n",
    "    \n",
    "    # Create request body for Titan embedding model\n",
    "    body = json.dumps({\n",
    "        \"inputImage\": image_base64,\n",
    "        \"embeddingConfig\": {\n",
    "            \"outputEmbeddingLength\": 1024\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # Exponential backoff retry strategy\n",
    "    retry_delays = [1, 2, 4, 8, 16]\n",
    "\n",
    "    for attempt, delay in enumerate(retry_delays + [None]):\n",
    "        print(f\"inovke Embedding - attempt {attempt}\", end = \"\\r\")\n",
    "        try:\n",
    "            # Invoke Titan embedding model\n",
    "            response = bedrock_client.invoke_model(\n",
    "                body=body,\n",
    "                modelId=\"amazon.titan-embed-image-v1\",\n",
    "                accept = \"application/json\",\n",
    "                contentType = \"application/json\"\n",
    "            )\n",
    "\n",
    "            # Extract embedding from response\n",
    "            temp = json.loads(response['body'].read())['embedding']\n",
    "\n",
    "            return np.array(temp).reshape(1, -1)\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke amazon.titan-embed-image-v1. Reason: {e}\")\n",
    "            if delay is not None:\n",
    "                time.sleep(delay)  # Wait before retry\n",
    "            else:\n",
    "                return None  # All retries exhausted\n",
    "    return None\n",
    "\n",
    "# Function to generate embeddings for multiple images in parallel\n",
    "def get_image_embeddings_parallel(\n",
    "        boto3_session, \n",
    "        frames, \n",
    "        max_workers=10\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generate embeddings for multiple images in parallel using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        boto3_session: AWS boto3 session for API calls\n",
    "        frames (list): List of extracted frames\n",
    "        max_workers (int): Maximum number of parallel threads (default: 10)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Stacked embeddings array of shape (num_images, 1024)\n",
    "    \"\"\"\n",
    "    image_base64_list = []\n",
    "    # Convert frames to base64\n",
    "    for frame in frames:\n",
    "        pil_img = Image.fromarray(frame)\n",
    "        img_buffer = BytesIO()\n",
    "        pil_img.save(img_buffer, format=\"PNG\")\n",
    "        img_buffer.seek(0)\n",
    "        img_str = base64.b64encode(img_buffer.read()).decode('utf-8')\n",
    "        image_base64_list.append(img_str)\n",
    "\n",
    "    # Process images in parallel using thread pool\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        embeddings = list(\n",
    "            executor.map(\n",
    "                lambda image_base64: get_titan_embedding_image(\n",
    "                    boto3_session, \n",
    "                    image_base64\n",
    "                ), \n",
    "                image_base64_list\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Stack all embeddings into single array\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    \n",
    "    return embeddings.astype(np.float32)\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        a (np.ndarray): First vector\n",
    "        b (np.ndarray): Second vector\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity value between -1 and 1\n",
    "    \"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for semantic frame sampling using AI embeddings to detect scene changes\n",
    "def semantic_frame_sampling(\n",
    "        num_frames,\n",
    "        frames,\n",
    "        embeddings,\n",
    "        threshold=0.8\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Perform semantic frame sampling using Amazon Titan embeddings.\n",
    "\n",
    "    Args:\n",
    "        num_frames (int): Number of frames per segement\n",
    "        frames (list): List of video frames\n",
    "        embeddings (np.ndarray): Pre-computed embeddings for all frames\n",
    "        threshold (float): Cosine similarity threshold for scene segmentation (default: 0.8)\n",
    "\n",
    "    Returns:\n",
    "        list: List of sampled frames representing different scenes\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate cosine similarities between consecutive frames\n",
    "    similarities = []\n",
    "    for i in range(1, embeddings.shape[0]):\n",
    "        similarities.append(cosine_sim(embeddings[i-1], embeddings[i]))\n",
    "\n",
    "    # Find segment boundaries where similarity drops below threshold\n",
    "    segments = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        if sim < threshold:  # Scene change detected\n",
    "            segments.append((start_idx, i + 1))\n",
    "            start_idx = i + 1\n",
    "    \n",
    "    # Add the final segment\n",
    "    if start_idx < len(embeddings):\n",
    "        segments.append((start_idx, len(embeddings)))\n",
    "    \n",
    "    # Uniformly sample frames within each detected segment\n",
    "    frame_indices = []\n",
    "    for seg_idx, (start, end) in enumerate(segments):\n",
    "        seg_len = end - start\n",
    "        # Distribute frames evenly across segments\n",
    "        seg_samples = max(1, min(num_frames, seg_len))\n",
    "        \n",
    "        # Create uniform intervals within segment\n",
    "        intervals = np.linspace(start, end, seg_samples + 1).astype(int)\n",
    "        # Select middle frame from each interval\n",
    "        seg_indices = [(intervals[i] + intervals[i + 1] - 1) // 2 for i in range(len(intervals) - 1)]\n",
    "\n",
    "        display_string = \",\".join([str(x) for x in seg_indices])\n",
    "        print(f\"Segment {seg_idx}: {display_string}\")\n",
    "        \n",
    "        frame_indices += seg_indices\n",
    "\n",
    "    return [frames[idx] for idx in frame_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all frames using Amazon Titan\n",
    "# This step converts each frame into a 1024-dimensional vector representation\n",
    "print(f\"ðŸ§  Generating embeddings for {len(frames_long)} frames...\")\n",
    "embeddings = get_image_embeddings_parallel(\n",
    "        session, \n",
    "        frames_long, \n",
    "        max_workers=10\n",
    "    )\n",
    "print(f\"âœ… Generated embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue' size = 4>Now we have generated embeddings for all frames. Next, we need to set a threshold of similarities between consecutive frames, so we could use this threshold to determine the boundary of different scene.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES_PER_SEGMENT = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='red'>Feel free adjust the threshold value below and see how different threshold impact video segementation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "print(f\"ðŸ¤– Applying semantic sampling with threshold={threshold}...\")\n",
    "semantic_sampled_frames = semantic_frame_sampling(\n",
    "        NUM_FRAMES_PER_SEGMENT,\n",
    "        frames_long,\n",
    "        embeddings,\n",
    "        threshold=threshold\n",
    "    )\n",
    "print(f\"ðŸŽ¬ Semantic sampling selected {len(semantic_sampled_frames)} frames,\\n\\t{int(len(semantic_sampled_frames)/NUM_FRAMES_PER_SEGMENT)} segments,\\n\\t{NUM_FRAMES_PER_SEGMENT} for each segment\")\n",
    "visualize_frames(\n",
    "    semantic_sampled_frames,\n",
    "    max_frames_per_row = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 0.8\n",
    "print(f\"ðŸ¤– Applying semantic sampling with threshold={threshold}...\")\n",
    "semantic_sampled_frames = semantic_frame_sampling(\n",
    "        NUM_FRAMES_PER_SEGMENT,\n",
    "        frames_long,\n",
    "        embeddings,\n",
    "        threshold=threshold\n",
    "    )\n",
    "print(f\"ðŸŽ¬ Semantic sampling selected {len(semantic_sampled_frames)} frames,\\n\\t{int(len(semantic_sampled_frames)/NUM_FRAMES_PER_SEGMENT)} segments,\\n\\t{NUM_FRAMES_PER_SEGMENT} for each segment\")\n",
    "visualize_frames(\n",
    "    semantic_sampled_frames,\n",
    "    max_frames_per_row = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply semantic sampling using AI-powered scene detection\n",
    "# Lower threshold = more sensitive to scene changes (more segments)\n",
    "# Higher threshold = less sensitive (fewer segments)\n",
    "threshold = 0.9\n",
    "print(f\"ðŸ¤– Applying semantic sampling with threshold={threshold}...\")\n",
    "semantic_sampled_frames = semantic_frame_sampling(\n",
    "        NUM_FRAMES_PER_SEGMENT,\n",
    "        frames_long,\n",
    "        embeddings,\n",
    "        threshold=threshold\n",
    "    )\n",
    "print(f\"ðŸŽ¬ Semantic sampling selected {len(semantic_sampled_frames)} frames,\\n\\t{int(len(semantic_sampled_frames)/NUM_FRAMES_PER_SEGMENT)} segments,\\n\\t{NUM_FRAMES_PER_SEGMENT} for each segment\")\n",
    "visualize_frames(\n",
    "    semantic_sampled_frames,\n",
    "    max_frames_per_row = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Complete Video Processing Pipeline - Putting It All Together\n",
    "\n",
    "**Important Note**: Amazon Nova models have built-in video processing capabilities that automatically handle uniform frame sampling when videos are loaded for analysis. For videos less than 16 minutes, the default sampling rate is 1 frame per second. \n",
    "\n",
    "\n",
    "The `process_videos_pipeline()` function serves as the second component of the ROBO-reviewer, downloading videos from S3, performing intelligent scene segmentation, and uploading structured results back to S3. Since Nova handles frame sampling internally (https://docs.aws.amazon.com/nova/latest/userguide/modalities-video.html), this pipeline focuses exclusively on scene segmentation - a critical capability for evaluating videos containing multiple distinct scenes where individual scene analysis provides more accurate assessment results.\n",
    "\n",
    "\n",
    "\n",
    "**Key Operations**:\n",
    "\n",
    "2. **AI-driven scene segmentation** - Uses Amazon Titan embeddings to automatically detect scene changes based on visual similarity thresholds\n",
    "3. **Batch S3 processing** - Downloads videos from S3 URIs, processes locally, and uploads structured results back to S3\n",
    "4. **Organized output structure** - Creates systematic folder hierarchy with separate videos for each sampling method and detected scene segment\n",
    "\n",
    "  \n",
    "\n",
    "**Output Structure**:\n",
    "\n",
    "The pipeline focuses on semantic segmentation since Nova handles uniform/random sampling automatically. Results are saved to S3 with this structure:\n",
    "\n",
    "```\n",
    "s3://<your bucket>/\n",
    "â””â”€â”€ generated_videos/\n",
    "    â”œâ”€â”€ <generated video file>.mp4\n",
    "    â””â”€â”€ <generated video file>/\n",
    "        â””â”€â”€ semantic/                    # Semantic segmentation results\n",
    "            â”œâ”€â”€ 0_video.mp4             # Segment 0 - Complete scene segment\n",
    "            â”œâ”€â”€ 1_video.mp4             # Segment 1 - Complete scene segment\n",
    "            â”œâ”€â”€ 2_video.mp4             # Segment 2 - Complete scene segment\n",
    "            â””â”€â”€ ...                     # Additional segments as detected\n",
    "```\n",
    "\n",
    "**Note**: Uniform and random sampling outputs are commented out in the pipeline since Nova models perform these operations automatically with optimized strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get S3 bucket name\n",
    "S3_BUCKET = get_s3_bucket(session)\n",
    "\n",
    "# Load configuration for video prefix\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "available_videos = discover_video_files(\n",
    "    session, \n",
    "    S3_BUCKET, \n",
    "    config['video_prefix']\n",
    ")\n",
    "\n",
    "# Example S3 URIs - replace with your actual video names\n",
    "video_names = available_videos[0:2]  # Replace with your video names\n",
    "s3_uris = [f\"s3://{S3_BUCKET}/{config['video_prefix']}{name}\" for name in video_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_videos_pipeline(\n",
    "    session,\n",
    "    s3_uris,\n",
    "    num_frames = 16,  # number of sampled frames\n",
    "    threshold = 0.9  # threshold for segmenting video through semantic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully learned about:\n",
    "\n",
    "1. **Amazon Nova's automatic sampling** - Understanding how Nova models handle video processing internally\n",
    "2. **Traditional sampling methods** (uniform/random) for analysis and comparison\n",
    "3. **Semantic sampling** for intelligent scene segmentation and content-aware processing\n",
    "4. **Production pipeline** that creates structured video segments for downstream evaluation\n",
    "\n",
    "Now you can move to the next module to see how to evaluate the quality of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
